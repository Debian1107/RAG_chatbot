# ğŸ” Retrieval-Augmented Generation (RAG) with Gemini & PDF

This project implements a lightweight RAG (Retrieval-Augmented Generation) pipeline that allows you to ask questions about the contents of any PDF using Googleâ€™s Gemini LLM. It uses LangChain for loading and chunking documents, SentenceTransformers for embedding generation, and ChromaDB for vector storage and retrieval. The goal is to enable efficient semantic search and natural language Q&A over unstructured text documents.

---

## ğŸ“Œ Features

- ğŸ“„ Load and parse PDF documents
- âœ‚ï¸ Chunk large text into overlapping segments
- ğŸ§  Generate vector embeddings using `all-MiniLM-L6-v2`
- ğŸ’¾ Store and retrieve from a persistent vector DB (Chroma)
- ğŸ” Perform similarity search to fetch relevant context
- ğŸ’¬ Use Google Gemini to answer questions based on context

---

## âš™ï¸ Tech Stack

- **LangChain** â€“ document loader, text splitter
- **ChromaDB** â€“ vector storage and retrieval
- **SentenceTransformers** â€“ for embedding generation
- **Google Generative AI SDK** â€“ Gemini LLM
- **Python & dotenv** â€“ API key and environment setup

---

## ğŸš€ Getting Started

### 1. Clone the repository
```bash
git clone https://github.com/your-username/rag-gemini-pdf.git
cd rag-gemini-pdf
